{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_ranks(df,partition,metric):\n",
    "    df = df[df[\"partition\"]==partition]\n",
    "    df = df.sort_values(metric)\n",
    "    df[\"rank_\"+metric] = np.arange(len(df))\n",
    "    return df\n",
    "def get_total_rank(df,partition,metrics = [\"wmape\",\"dice\",\"dpeaks\"]):\n",
    "    for metric in metrics:\n",
    "        df = get_ranks(df,partition,metric)\n",
    "    df[\"total_rank\"] = df[\"rank_wmape\"] + df[\"rank_dice\"] + df[\"rank_dpeaks\"]\n",
    "    df = df.sort_values(\"total_rank\")\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized(df,metric,partition=None):\n",
    "    if partition:\n",
    "        df = df[df[\"partition\"]==partition].copy()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "    df[\"normalized_\"+metric] = (df[metric] - df[metric].min())/(df[metric].max()-df[metric].min())\n",
    "    return df\n",
    "def get_total_normalized(df,partition=None,metrics = [\"wmape\",\"dice\",\"dpeaks\"]):\n",
    "    for metric in metrics:\n",
    "        df = get_normalized(df,metric,partition)\n",
    "    df[\"total_normalized\"] = df[\"normalized_wmape\"] + df[\"normalized_dice\"] + df[\"normalized_dpeaks\"]\n",
    "    df = df.sort_values(\"total_normalized\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>partition</th>\n",
       "      <th>wmape</th>\n",
       "      <th>dice</th>\n",
       "      <th>dpeaks</th>\n",
       "      <th>loss</th>\n",
       "      <th>normalized_wmape</th>\n",
       "      <th>normalized_dice</th>\n",
       "      <th>normalized_dpeaks</th>\n",
       "      <th>total_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>5</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>6</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.00451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.013015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>4</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>33.5</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.010846</td>\n",
       "      <td>0.025680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unet_mae_magandor_lr</td>\n",
       "      <td>3</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.00542</td>\n",
       "      <td>0.082126</td>\n",
       "      <td>0.106897</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.223729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unet_mae_magandor_lr</td>\n",
       "      <td>4</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>38.6</td>\n",
       "      <td>0.00551</td>\n",
       "      <td>0.094203</td>\n",
       "      <td>0.117241</td>\n",
       "      <td>0.121475</td>\n",
       "      <td>0.332919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>5</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>42.7</td>\n",
       "      <td>0.00547</td>\n",
       "      <td>0.084541</td>\n",
       "      <td>0.077586</td>\n",
       "      <td>0.210412</td>\n",
       "      <td>0.372539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>4</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.024155</td>\n",
       "      <td>0.084483</td>\n",
       "      <td>0.277657</td>\n",
       "      <td>0.386295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>3</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>51.8</td>\n",
       "      <td>0.00603</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.284483</td>\n",
       "      <td>0.407809</td>\n",
       "      <td>0.800988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>3</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>56.6</td>\n",
       "      <td>0.00606</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.460345</td>\n",
       "      <td>0.511931</td>\n",
       "      <td>1.083387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>2</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.00627</td>\n",
       "      <td>0.135266</td>\n",
       "      <td>0.463793</td>\n",
       "      <td>0.542299</td>\n",
       "      <td>1.141358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>1</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>54.3</td>\n",
       "      <td>0.00709</td>\n",
       "      <td>0.176329</td>\n",
       "      <td>0.529310</td>\n",
       "      <td>0.462039</td>\n",
       "      <td>1.167678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>1</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>59.1</td>\n",
       "      <td>0.00697</td>\n",
       "      <td>0.096618</td>\n",
       "      <td>0.574138</td>\n",
       "      <td>0.566161</td>\n",
       "      <td>1.236917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>atunception_mae_magandorgraderr_lr</td>\n",
       "      <td>2</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.0736</td>\n",
       "      <td>44.7</td>\n",
       "      <td>0.01180</td>\n",
       "      <td>0.570048</td>\n",
       "      <td>0.527586</td>\n",
       "      <td>0.253796</td>\n",
       "      <td>1.351431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>atunception_mae_magandorgraderr_lr</td>\n",
       "      <td>3</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>41.4</td>\n",
       "      <td>0.01580</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.737931</td>\n",
       "      <td>0.182213</td>\n",
       "      <td>1.376665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unet_mae_magandor_lr</td>\n",
       "      <td>2</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.508621</td>\n",
       "      <td>0.626898</td>\n",
       "      <td>1.642765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>atunception_mae_magandorgraderr_lr</td>\n",
       "      <td>4</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>41.3</td>\n",
       "      <td>0.01600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641379</td>\n",
       "      <td>0.180043</td>\n",
       "      <td>1.821423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unet_mae_magandor_lr</td>\n",
       "      <td>1</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>66.4</td>\n",
       "      <td>0.00858</td>\n",
       "      <td>0.350242</td>\n",
       "      <td>0.891379</td>\n",
       "      <td>0.724512</td>\n",
       "      <td>1.966133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>atunception_mae_magandorgraderr_lr</td>\n",
       "      <td>1</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.0914</td>\n",
       "      <td>58.8</td>\n",
       "      <td>0.02380</td>\n",
       "      <td>0.850242</td>\n",
       "      <td>0.834483</td>\n",
       "      <td>0.559653</td>\n",
       "      <td>2.244377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>2</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>79.1</td>\n",
       "      <td>0.00894</td>\n",
       "      <td>0.350242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.350242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   exp  partition  wmape    dice  dpeaks  \\\n",
       "13         unet_mae_magandorgraderr_lr          5   1.51  0.0433    33.0   \n",
       "14         unet_mae_magandorgraderr_lr          6   1.51  0.0430    33.6   \n",
       "12         unet_mae_magandorgraderr_lr          4   1.55  0.0433    33.5   \n",
       "1                 unet_mae_magandor_lr          3   1.85  0.0492    34.6   \n",
       "0                 unet_mae_magandor_lr          4   1.90  0.0498    38.6   \n",
       "8        atunet_mae_magandorgraderr_lr          5   1.86  0.0475    42.7   \n",
       "7        atunet_mae_magandorgraderr_lr          4   1.61  0.0479    45.8   \n",
       "6        atunet_mae_magandorgraderr_lr          3   1.96  0.0595    51.8   \n",
       "11         unet_mae_magandorgraderr_lr          3   1.97  0.0697    56.6   \n",
       "5        atunet_mae_magandorgraderr_lr          2   2.07  0.0699    58.0   \n",
       "4        atunet_mae_magandorgraderr_lr          1   2.24  0.0737    54.3   \n",
       "9          unet_mae_magandorgraderr_lr          1   1.91  0.0763    59.1   \n",
       "16  atunception_mae_magandorgraderr_lr          2   3.87  0.0736    44.7   \n",
       "17  atunception_mae_magandorgraderr_lr          3   3.40  0.0858    41.4   \n",
       "2                 unet_mae_magandor_lr          2   3.61  0.0725    61.9   \n",
       "18  atunception_mae_magandorgraderr_lr          4   5.65  0.0802    41.3   \n",
       "3                 unet_mae_magandor_lr          1   2.96  0.0947    66.4   \n",
       "15  atunception_mae_magandorgraderr_lr          1   5.03  0.0914    58.8   \n",
       "10         unet_mae_magandorgraderr_lr          2   2.96  0.1010    79.1   \n",
       "\n",
       "       loss  normalized_wmape  normalized_dice  normalized_dpeaks  \\\n",
       "13  0.00460          0.000000         0.005172           0.000000   \n",
       "14  0.00451          0.000000         0.000000           0.013015   \n",
       "12  0.00459          0.009662         0.005172           0.010846   \n",
       "1   0.00542          0.082126         0.106897           0.034707   \n",
       "0   0.00551          0.094203         0.117241           0.121475   \n",
       "8   0.00547          0.084541         0.077586           0.210412   \n",
       "7   0.00502          0.024155         0.084483           0.277657   \n",
       "6   0.00603          0.108696         0.284483           0.407809   \n",
       "11  0.00606          0.111111         0.460345           0.511931   \n",
       "5   0.00627          0.135266         0.463793           0.542299   \n",
       "4   0.00709          0.176329         0.529310           0.462039   \n",
       "9   0.00697          0.096618         0.574138           0.566161   \n",
       "16  0.01180          0.570048         0.527586           0.253796   \n",
       "17  0.01580          0.456522         0.737931           0.182213   \n",
       "2   0.01100          0.507246         0.508621           0.626898   \n",
       "18  0.01600          1.000000         0.641379           0.180043   \n",
       "3   0.00858          0.350242         0.891379           0.724512   \n",
       "15  0.02380          0.850242         0.834483           0.559653   \n",
       "10  0.00894          0.350242         1.000000           1.000000   \n",
       "\n",
       "    total_normalized  \n",
       "13          0.005172  \n",
       "14          0.013015  \n",
       "12          0.025680  \n",
       "1           0.223729  \n",
       "0           0.332919  \n",
       "8           0.372539  \n",
       "7           0.386295  \n",
       "6           0.800988  \n",
       "11          1.083387  \n",
       "5           1.141358  \n",
       "4           1.167678  \n",
       "9           1.236917  \n",
       "16          1.351431  \n",
       "17          1.376665  \n",
       "2           1.642765  \n",
       "18          1.821423  \n",
       "3           1.966133  \n",
       "15          2.244377  \n",
       "10          2.350242  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"logs.csv\")\n",
    "get_total_normalized(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101218/220033433.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(\"../experimentos/unet_mae_magandorgraderr_lr/models/best_model_partition_4.pth\",map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"../experimentos/unet_mae_magandorgraderr_lr/models/best_model_partition_4.pth\",map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model_state_dict[\"encoder1.conv.0.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel importance (normalized): [0.10824206 0.10983228 0.10713389 0.11062915 0.1092234  0.11005307\n",
      " 0.11367284 0.11272318 0.11849003]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Assuming `weights` is a numpy array of shape (channels_out, channels_in, kernel_height, kernel_width)\n",
    "# weights.shape -> (64, 9, kh, kw)\n",
    "\n",
    "# Compute the sum of absolute values of weights for each input channel\n",
    "channel_importance = np.sum(np.abs(w.detach().numpy()), axis=(0, 2, 3))\n",
    "\n",
    "# Normalize or sort to identify the most significant channels\n",
    "normalized_importance = channel_importance / np.sum(channel_importance)\n",
    "print(\"Channel importance (normalized):\", normalized_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.011356148)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(channel_importance.min()-channel_importance.max())/ np.sum(channel_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
