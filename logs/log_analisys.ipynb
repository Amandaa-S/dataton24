{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_ranks(df,partition,metric):\n",
    "    df = df[df[\"partition\"]==partition]\n",
    "    df = df.sort_values(metric)\n",
    "    df[\"rank_\"+metric] = np.arange(len(df))\n",
    "    return df\n",
    "def get_total_rank(df,partition,metrics = [\"wmape\",\"dice\",\"dpeaks\"]):\n",
    "    for metric in metrics:\n",
    "        df = get_ranks(df,partition,metric)\n",
    "    df[\"total_rank\"] = df[\"rank_wmape\"] + df[\"rank_dice\"] + df[\"rank_dpeaks\"]\n",
    "    df = df.sort_values(\"total_rank\")\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized(df,metric,partition=None):\n",
    "    if partition:\n",
    "        df = df[df[\"partition\"]==partition].copy()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "    df[\"normalized_\"+metric] = (df[metric] - df[metric].min())/(df[metric].max()-df[metric].min())\n",
    "    return df\n",
    "def get_total_normalized(df,partition=None,metrics = [\"wmape\",\"dice\",\"dpeaks\"]):\n",
    "    for metric in metrics:\n",
    "        df = get_normalized(df,metric,partition)\n",
    "    df[\"total_normalized\"] = df[\"normalized_wmape\"] + df[\"normalized_dice\"] + df[\"normalized_dpeaks\"]\n",
    "    df = df.sort_values(\"total_normalized\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>partition</th>\n",
       "      <th>wmape</th>\n",
       "      <th>dice</th>\n",
       "      <th>dpeaks</th>\n",
       "      <th>loss</th>\n",
       "      <th>normalized_wmape</th>\n",
       "      <th>normalized_dice</th>\n",
       "      <th>normalized_dpeaks</th>\n",
       "      <th>total_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>unet_mae_magandorgraderr_lr_totalpartition_bug...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.00421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>5</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.041425</td>\n",
       "      <td>0.078526</td>\n",
       "      <td>0.143536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>6</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.00451</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.038940</td>\n",
       "      <td>0.088141</td>\n",
       "      <td>0.150665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>4</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>33.5</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>0.033019</td>\n",
       "      <td>0.041425</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.160982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unet_mae_magandor_lr</td>\n",
       "      <td>3</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.00542</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.090307</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.298247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>unet_mae_magandorgraderr_lr_totalpartition</td>\n",
       "      <td>3</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>38.4</td>\n",
       "      <td>0.00553</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>0.096106</td>\n",
       "      <td>0.165064</td>\n",
       "      <td>0.346076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unet_mae_magandor_lr</td>\n",
       "      <td>4</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>38.6</td>\n",
       "      <td>0.00551</td>\n",
       "      <td>0.115566</td>\n",
       "      <td>0.095278</td>\n",
       "      <td>0.168269</td>\n",
       "      <td>0.379113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>unet_mae_magandorgrad_lr_totalpartition_autoca...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.096698</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.174679</td>\n",
       "      <td>0.398138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>4</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.079536</td>\n",
       "      <td>0.283654</td>\n",
       "      <td>0.410360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>5</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>42.7</td>\n",
       "      <td>0.00547</td>\n",
       "      <td>0.106132</td>\n",
       "      <td>0.076222</td>\n",
       "      <td>0.233974</td>\n",
       "      <td>0.416328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>unet_mae_magandorgrad_lr_totalpartition_autoca...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>40.7</td>\n",
       "      <td>0.00570</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.128418</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.443548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>unet_mae_magandorgraderr_lr_totalpartition</td>\n",
       "      <td>2</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.00540</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.164872</td>\n",
       "      <td>0.334936</td>\n",
       "      <td>0.598864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>3</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>51.8</td>\n",
       "      <td>0.00603</td>\n",
       "      <td>0.129717</td>\n",
       "      <td>0.175642</td>\n",
       "      <td>0.379808</td>\n",
       "      <td>0.685167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>3</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>56.6</td>\n",
       "      <td>0.00606</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.260149</td>\n",
       "      <td>0.456731</td>\n",
       "      <td>0.848955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>2</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.00627</td>\n",
       "      <td>0.155660</td>\n",
       "      <td>0.261806</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.896633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atunet_mae_magandorgraderr_lr</td>\n",
       "      <td>1</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>54.3</td>\n",
       "      <td>0.00709</td>\n",
       "      <td>0.195755</td>\n",
       "      <td>0.293289</td>\n",
       "      <td>0.419872</td>\n",
       "      <td>0.908916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>1</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>59.1</td>\n",
       "      <td>0.00697</td>\n",
       "      <td>0.117925</td>\n",
       "      <td>0.314830</td>\n",
       "      <td>0.496795</td>\n",
       "      <td>0.929550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>unet_mae_magandorgraderr_lr_totalpartition</td>\n",
       "      <td>1</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>59.1</td>\n",
       "      <td>0.00697</td>\n",
       "      <td>0.117925</td>\n",
       "      <td>0.314830</td>\n",
       "      <td>0.496795</td>\n",
       "      <td>0.929550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>atunception_mae_magandorgraderr_lr</td>\n",
       "      <td>3</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>41.4</td>\n",
       "      <td>0.01580</td>\n",
       "      <td>0.469340</td>\n",
       "      <td>0.393538</td>\n",
       "      <td>0.213141</td>\n",
       "      <td>1.076018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>unet_mae_magandorgrad_lr_totalpartition_autocast</td>\n",
       "      <td>1</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>60.4</td>\n",
       "      <td>0.00721</td>\n",
       "      <td>0.186321</td>\n",
       "      <td>0.425021</td>\n",
       "      <td>0.517628</td>\n",
       "      <td>1.128970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>atunception_mae_magandorgraderr_lr</td>\n",
       "      <td>2</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.0736</td>\n",
       "      <td>44.7</td>\n",
       "      <td>0.01180</td>\n",
       "      <td>0.580189</td>\n",
       "      <td>0.292461</td>\n",
       "      <td>0.266026</td>\n",
       "      <td>1.138675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unet_mae_magandor_lr</td>\n",
       "      <td>2</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.518868</td>\n",
       "      <td>0.283347</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>1.343882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unet_mae_magandor_lr</td>\n",
       "      <td>1</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>66.4</td>\n",
       "      <td>0.00858</td>\n",
       "      <td>0.365566</td>\n",
       "      <td>0.467274</td>\n",
       "      <td>0.613782</td>\n",
       "      <td>1.446622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>atunception_mae_magandorgraderr_lr</td>\n",
       "      <td>4</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>41.3</td>\n",
       "      <td>0.01600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347142</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>1.558680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unet_mae_magandorgraderr_lr</td>\n",
       "      <td>2</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>79.1</td>\n",
       "      <td>0.00894</td>\n",
       "      <td>0.365566</td>\n",
       "      <td>0.519470</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>1.702343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>atunception_mae_magandorgraderr_lr</td>\n",
       "      <td>1</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.0914</td>\n",
       "      <td>58.8</td>\n",
       "      <td>0.02380</td>\n",
       "      <td>0.853774</td>\n",
       "      <td>0.439934</td>\n",
       "      <td>0.491987</td>\n",
       "      <td>1.785694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>unet_mae_6features_lr_5parttion_autocast</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>73.6</td>\n",
       "      <td>0.00950</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.693455</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>1.797622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>miniunet_mae_magandorgrar_deep2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>80.6</td>\n",
       "      <td>0.00988</td>\n",
       "      <td>0.360849</td>\n",
       "      <td>0.776305</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>1.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>miniunet_mae_magandorgrar_deep2_less_channels</td>\n",
       "      <td>1</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>90.5</td>\n",
       "      <td>0.01130</td>\n",
       "      <td>0.518868</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.518868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  exp  partition  wmape  \\\n",
       "22  unet_mae_magandorgraderr_lr_totalpartition_bug...          4   1.41   \n",
       "13                        unet_mae_magandorgraderr_lr          5   1.51   \n",
       "14                        unet_mae_magandorgraderr_lr          6   1.51   \n",
       "12                        unet_mae_magandorgraderr_lr          4   1.55   \n",
       "1                                unet_mae_magandor_lr          3   1.85   \n",
       "21         unet_mae_magandorgraderr_lr_totalpartition          3   1.77   \n",
       "0                                unet_mae_magandor_lr          4   1.90   \n",
       "25  unet_mae_magandorgrad_lr_totalpartition_autoca...          2   1.82   \n",
       "7                       atunet_mae_magandorgraderr_lr          4   1.61   \n",
       "8                       atunet_mae_magandorgraderr_lr          5   1.86   \n",
       "24  unet_mae_magandorgrad_lr_totalpartition_autoca...          1   1.89   \n",
       "20         unet_mae_magandorgraderr_lr_totalpartition          2   1.83   \n",
       "6                       atunet_mae_magandorgraderr_lr          3   1.96   \n",
       "11                        unet_mae_magandorgraderr_lr          3   1.97   \n",
       "5                       atunet_mae_magandorgraderr_lr          2   2.07   \n",
       "4                       atunet_mae_magandorgraderr_lr          1   2.24   \n",
       "9                         unet_mae_magandorgraderr_lr          1   1.91   \n",
       "19         unet_mae_magandorgraderr_lr_totalpartition          1   1.91   \n",
       "17                 atunception_mae_magandorgraderr_lr          3   3.40   \n",
       "23   unet_mae_magandorgrad_lr_totalpartition_autocast          1   2.20   \n",
       "16                 atunception_mae_magandorgraderr_lr          2   3.87   \n",
       "2                                unet_mae_magandor_lr          2   3.61   \n",
       "3                                unet_mae_magandor_lr          1   2.96   \n",
       "18                 atunception_mae_magandorgraderr_lr          4   5.65   \n",
       "10                        unet_mae_magandorgraderr_lr          2   2.96   \n",
       "15                 atunception_mae_magandorgraderr_lr          1   5.03   \n",
       "26           unet_mae_6features_lr_5parttion_autocast          1   3.00   \n",
       "27                    miniunet_mae_magandorgrar_deep2          1   2.94   \n",
       "28      miniunet_mae_magandorgrar_deep2_less_channels          1   3.61   \n",
       "\n",
       "      dice  dpeaks     loss  normalized_wmape  normalized_dice  \\\n",
       "22  0.0383    28.1  0.00421          0.000000         0.000000   \n",
       "13  0.0433    33.0  0.00460          0.023585         0.041425   \n",
       "14  0.0430    33.6  0.00451          0.023585         0.038940   \n",
       "12  0.0433    33.5  0.00459          0.033019         0.041425   \n",
       "1   0.0492    34.6  0.00542          0.103774         0.090307   \n",
       "21  0.0499    38.4  0.00553          0.084906         0.096106   \n",
       "0   0.0498    38.6  0.00551          0.115566         0.095278   \n",
       "25  0.0536    39.0  0.00555          0.096698         0.126761   \n",
       "7   0.0479    45.8  0.00502          0.047170         0.079536   \n",
       "8   0.0475    42.7  0.00547          0.106132         0.076222   \n",
       "24  0.0538    40.7  0.00570          0.113208         0.128418   \n",
       "20  0.0582    49.0  0.00540          0.099057         0.164872   \n",
       "6   0.0595    51.8  0.00603          0.129717         0.175642   \n",
       "11  0.0697    56.6  0.00606          0.132075         0.260149   \n",
       "5   0.0699    58.0  0.00627          0.155660         0.261806   \n",
       "4   0.0737    54.3  0.00709          0.195755         0.293289   \n",
       "9   0.0763    59.1  0.00697          0.117925         0.314830   \n",
       "19  0.0763    59.1  0.00697          0.117925         0.314830   \n",
       "17  0.0858    41.4  0.01580          0.469340         0.393538   \n",
       "23  0.0896    60.4  0.00721          0.186321         0.425021   \n",
       "16  0.0736    44.7  0.01180          0.580189         0.292461   \n",
       "2   0.0725    61.9  0.01100          0.518868         0.283347   \n",
       "3   0.0947    66.4  0.00858          0.365566         0.467274   \n",
       "18  0.0802    41.3  0.01600          1.000000         0.347142   \n",
       "10  0.1010    79.1  0.00894          0.365566         0.519470   \n",
       "15  0.0914    58.8  0.02380          0.853774         0.439934   \n",
       "26  0.1220    73.6  0.00950          0.375000         0.693455   \n",
       "27  0.1320    80.6  0.00988          0.360849         0.776305   \n",
       "28  0.1590    90.5  0.01130          0.518868         1.000000   \n",
       "\n",
       "    normalized_dpeaks  total_normalized  \n",
       "22           0.000000          0.000000  \n",
       "13           0.078526          0.143536  \n",
       "14           0.088141          0.150665  \n",
       "12           0.086538          0.160982  \n",
       "1            0.104167          0.298247  \n",
       "21           0.165064          0.346076  \n",
       "0            0.168269          0.379113  \n",
       "25           0.174679          0.398138  \n",
       "7            0.283654          0.410360  \n",
       "8            0.233974          0.416328  \n",
       "24           0.201923          0.443548  \n",
       "20           0.334936          0.598864  \n",
       "6            0.379808          0.685167  \n",
       "11           0.456731          0.848955  \n",
       "5            0.479167          0.896633  \n",
       "4            0.419872          0.908916  \n",
       "9            0.496795          0.929550  \n",
       "19           0.496795          0.929550  \n",
       "17           0.213141          1.076018  \n",
       "23           0.517628          1.128970  \n",
       "16           0.266026          1.138675  \n",
       "2            0.541667          1.343882  \n",
       "3            0.613782          1.446622  \n",
       "18           0.211538          1.558680  \n",
       "10           0.817308          1.702343  \n",
       "15           0.491987          1.785694  \n",
       "26           0.729167          1.797622  \n",
       "27           0.841346          1.978500  \n",
       "28           1.000000          2.518868  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"logs.csv\")\n",
    "get_total_normalized(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101218/220033433.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(\"../experimentos/unet_mae_magandorgraderr_lr/models/best_model_partition_4.pth\",map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"../experimentos/unet_mae_magandorgraderr_lr/models/best_model_partition_4.pth\",map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model_state_dict[\"encoder1.conv.0.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel importance (normalized): [0.10824206 0.10983228 0.10713389 0.11062915 0.1092234  0.11005307\n",
      " 0.11367284 0.11272318 0.11849003]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Assuming `weights` is a numpy array of shape (channels_out, channels_in, kernel_height, kernel_width)\n",
    "# weights.shape -> (64, 9, kh, kw)\n",
    "\n",
    "# Compute the sum of absolute values of weights for each input channel\n",
    "channel_importance = np.sum(np.abs(w.detach().numpy()), axis=(0, 2, 3))\n",
    "\n",
    "# Normalize or sort to identify the most significant channels\n",
    "normalized_importance = channel_importance / np.sum(channel_importance)\n",
    "print(\"Channel importance (normalized):\", normalized_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.011356148)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(channel_importance.min()-channel_importance.max())/ np.sum(channel_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.purrfect.dataset import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
