{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga modulo comun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('../../common'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from purrfect.dataset import load_partition,save_partition, create_train_valid_loaders\n",
        "\n",
        "from purrfect.training import train_model\n",
        "import torch.optim as optim\n",
        "from purrfect.active_learning import create_next_partitions, test_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from purrfect.submission import create_submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definici√≥n modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChannelAdder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChannelAdder, self).__init__()\n",
        "        # Define Sobel and Laplacian kernels as tensors\n",
        "        self.sobel_x = torch.tensor([[-1., 0., 1.],\n",
        "                                     [-2., 0., 2.],\n",
        "                                     [-1., 0., 1.]], dtype=torch.float32,device=DEVICE).unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        self.sobel_y = torch.tensor([[-1., -2., -1.],\n",
        "                                     [ 0.,  0.,  0.],\n",
        "                                     [ 1.,  2.,  1.]], dtype=torch.float32,device=DEVICE).unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        self.laplacian_kernel = torch.tensor([[0.,  1., 0.],\n",
        "                                              [1., -4., 1.],\n",
        "                                              [0.,  1., 0.]], dtype=torch.float32,device=DEVICE).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        epsilon = 1e-8\n",
        "\n",
        "        # Extract the first, second, and third channels\n",
        "        first_channel = x[:, 0, :, :].unsqueeze(1)  # e1 (first channel)\n",
        "        second_channel = x[:, 1, :, :].unsqueeze(1)  # e2 (second channel)\n",
        "        third_channel = x[:, 2, :, :].unsqueeze(1)  # error (third channel)\n",
        "\n",
        "        # 1. Compute the first new channel: sqrt(first_channel^2 + second_channel^2)\n",
        "        new_channel1 = torch.sqrt(first_channel**2 + second_channel**2)\n",
        "\n",
        "        # 2. Compute the second new channel: 1/2 * arctan(channel2 / channel1)\n",
        "        new_channel2 = 0.5 * torch.atan(second_channel / (first_channel + epsilon))\n",
        "\n",
        "        # 3. Compute Sobel gradients and Laplacians for e1 (first_channel)\n",
        "        grad_e1_x = F.conv2d(first_channel, self.sobel_x, padding=1)\n",
        "        grad_e1_y = F.conv2d(first_channel, self.sobel_y, padding=1)\n",
        "        grad_e1_magnitude = torch.sqrt(grad_e1_x**2 + grad_e1_y**2)\n",
        "        #laplacian_e1 = F.conv2d(first_channel, self.laplacian_kernel, padding=1)\n",
        "\n",
        "        # 4. Compute Sobel gradients and Laplacians for e2 (second_channel)\n",
        "        grad_e2_x = F.conv2d(second_channel, self.sobel_x, padding=1)\n",
        "        grad_e2_y = F.conv2d(second_channel, self.sobel_y, padding=1)\n",
        "        grad_e2_magnitude = torch.sqrt(grad_e2_x**2 + grad_e2_y**2)\n",
        "        #laplacian_e2 = F.conv2d(second_channel, self.laplacian_kernel, padding=1)\n",
        "\n",
        "        # 5. Compute weighted ellipticity channels (e1_weighted, e2_weighted)\n",
        "        e1_weighted = first_channel / (third_channel + epsilon)\n",
        "        e2_weighted = second_channel / (third_channel + epsilon)\n",
        "\n",
        "        # Concatenate all the channels (original and new) into the output tensor\n",
        "        output = torch.cat([\n",
        "            x,                 # Original 3 channels\n",
        "            new_channel1,      # sqrt(channel1^2 + channel2^2)\n",
        "            new_channel2,      # 1/2 * arctan(channel2 / channel1)\n",
        "            #grad_e1_x,         # Gradient X of channel1\n",
        "            #grad_e1_y,         # Gradient Y of channel1\n",
        "            #grad_e2_x,         # Gradient X of channel2\n",
        "            #grad_e2_y,         # Gradient Y of channel2\n",
        "            grad_e1_magnitude, # Gradient magnitude of channel1\n",
        "            grad_e2_magnitude, # Gradient magnitude of channel2\n",
        "            #laplacian_e1,      # Laplacian of channel1\n",
        "            #laplacian_e2,      # Laplacian of channel2\n",
        "            e1_weighted,       # e1_weighted\n",
        "            e2_weighted        # e2_weighted\n",
        "        ], dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Define the Inception block with Dilated Convolutions\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        # 1x1 convolution\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        # 3x3 convolution\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        # 5x5 convolution\n",
        "        self.branch5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=4, dilation=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        self.cast = nn.Conv2d(out_channels*3, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply each branch\n",
        "        branch1 = self.branch1(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch5 = self.branch5(x)\n",
        "\n",
        "        # Concatenate the outputs along the channel dimension\n",
        "        outputs = torch.cat([branch1, branch3, branch5], dim=1)\n",
        "        return self.cast(outputs)\n",
        "\n",
        "\n",
        "# Define Additive Attention\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(F_int),\n",
        "        )\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = F.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "# U-Net with Additive Attention, Inception, and Recurrence\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "        self.channel_adder = ChannelAdder()\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            InceptionBlock(in_channels+6, 64),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            InceptionBlock(64, 128),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            InceptionBlock(128, 256),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.encoder4 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            InceptionBlock(256, 512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.center = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            InceptionBlock(512, 1024),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.decoder4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            InceptionBlock(512, 512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.att4 = AttentionBlock(F_g=512, F_l=512, F_int=256)\n",
        "\n",
        "        self.decoder3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2*512, 256, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            InceptionBlock(256, 256),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.att3 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
        "\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2*256, 128, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            InceptionBlock(128, 128),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.att2 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
        "\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2*128, 64, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            InceptionBlock(64, 64),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.final = nn.Conv2d(2*64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_adder(x)\n",
        "        # Encoder path\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "        center = self.center(e4)\n",
        "\n",
        "        # Decoder path\n",
        "        d4 = self.decoder4(center)\n",
        "        e4 = self.att4(g=d4, x=e4)\n",
        "        d4 = torch.cat([e4, d4], dim=1)\n",
        "\n",
        "        d3 = self.decoder3(d4)\n",
        "        e3 = self.att3(g=d3, x=e3)\n",
        "        d3 = torch.cat([e3, d3], dim=1)\n",
        "\n",
        "        d2 = self.decoder2(d3)\n",
        "        e2 = self.att2(g=d2, x=e2)\n",
        "        d2 = torch.cat([e2, d2], dim=1)\n",
        "\n",
        "        d1 = self.decoder1(d2)\n",
        "        d1 = torch.cat([e1, d1], dim=1)\n",
        "\n",
        "        out = self.final(d1)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creaci√≥n particion inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Creacion de particiones train y valid\n",
        "init_partition = load_partition(\"partition_1.json\")\n",
        "train_partition, val_partition = train_test_split(init_partition, test_size=0.2, random_state=42)\n",
        "save_partition(\"partition_1_train.json\",\"partitions\",train_partition)\n",
        "save_partition(\"partition_1_val.json\",\"partitions\",val_partition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define model\n",
        "model = UNet( 3, 1)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rYf70y6AJmLe"
      },
      "outputs": [],
      "source": [
        "# Define Loss\n",
        "criterion = torch.nn.L1Loss()\n",
        "current_partition = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.38it/s, WMAPE=6.01, DICE=0.23, DPEAKS=106, Loss=0.0191]\n",
            "Validate Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.19it/s, WMAPE=4.23, DICE=0.166, DPEAKS=111, Loss=0.0132]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model\n",
            "Epoch [2/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.38it/s, WMAPE=3.34, DICE=0.127, DPEAKS=72, Loss=0.0105]\n",
            "Validate Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.16it/s, WMAPE=4.99, DICE=0.11, DPEAKS=62, Loss=0.0148]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.37it/s, WMAPE=3.23, DICE=0.123, DPEAKS=72.3, Loss=0.0103]\n",
            "Validate Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.17it/s, WMAPE=2.68, DICE=0.0902, DPEAKS=52.4, Loss=0.00786]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model\n",
            "Epoch [4/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.37it/s, WMAPE=2.63, DICE=0.0846, DPEAKS=51.8, Loss=0.00808]\n",
            "Validate Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.17it/s, WMAPE=2.45, DICE=0.0908, DPEAKS=57, Loss=0.00762]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model\n",
            "Epoch [5/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.37it/s, WMAPE=2.43, DICE=0.0779, DPEAKS=48, Loss=0.00765]\n",
            "Validate Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.18it/s, WMAPE=2.54, DICE=0.082, DPEAKS=51.3, Loss=0.00859]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.37it/s, WMAPE=2.34, DICE=0.0739, DPEAKS=44.3, Loss=0.00737]\n",
            "Validate Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.15it/s, WMAPE=4.8, DICE=0.0779, DPEAKS=44.1, Loss=0.0349]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:08<00:00,  2.39it/s, WMAPE=2.25, DICE=0.0697, DPEAKS=41.5, Loss=0.00697]\n",
            "Validate Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.15it/s, WMAPE=7.22, DICE=0.0731, DPEAKS=37.4, Loss=0.0176]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "early stopping: 3 epochs without improvement\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "train_loader, val_loader = create_train_valid_loaders(\n",
        "    f\"partition_{current_partition}_train.json\",\n",
        "    f\"partition_{current_partition}_val.json\",\n",
        "    \"partitions\",\n",
        "    batch_size=16,\n",
        ")\n",
        "best_model_path = os.path.join(\n",
        "    \"models\", f\"best_model_partition_{current_partition}.pth\"\n",
        ")\n",
        "last_checkpoint_path = os.path.join(\n",
        "    \"models\", f\"last_checkpoint_partition_{current_partition}.pth\"\n",
        ")\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    best_model_path,\n",
        "    last_checkpoint_path,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    num_epochs=50,\n",
        "    device=DEVICE,\n",
        "    early_stopping_patience=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validate Epoch test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1941/1941 [05:08<00:00,  6.29it/s, WMAPE=5.03, DICE=0.0914, DPEAKS=58.8, Loss=0.0238]\n"
          ]
        }
      ],
      "source": [
        "#Cargar mejor modelo de la particion actual\n",
        "model.load_state_dict(torch.load(best_model_path,weights_only=True))\n",
        "test_model(model,criterion,device=DEVICE,batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validate Epoch partition_2_train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:51<00:00,  5.91it/s, WMAPE=2.6, DICE=0.0911, DPEAKS=58.3, Loss=0.00836]\n",
            "Validate Epoch partition_2_val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.30it/s, WMAPE=2.57, DICE=0.0914, DPEAKS=61.7, Loss=0.00786]\n"
          ]
        }
      ],
      "source": [
        "create_next_partitions(current_partition,model,criterion,device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_partition=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 1:   0%|          | 0/307 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:01<00:00,  2.52it/s, WMAPE=3.2, DICE=0.11, DPEAKS=72.8, Loss=0.00969]\n",
            "Validate Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:11<00:00,  6.62it/s, WMAPE=2.76, DICE=0.0974, DPEAKS=74.2, Loss=0.00802]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model\n",
            "Epoch [2/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:02<00:00,  2.51it/s, WMAPE=2.76, DICE=0.0837, DPEAKS=57.3, Loss=0.00812]\n",
            "Validate Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.15it/s, WMAPE=2.37, DICE=0.0729, DPEAKS=51, Loss=0.00762]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model\n",
            "Epoch [3/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:02<00:00,  2.50it/s, WMAPE=2.54, DICE=0.0745, DPEAKS=50.4, Loss=0.00738]\n",
            "Validate Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:11<00:00,  6.60it/s, WMAPE=2.79, DICE=0.0847, DPEAKS=54.7, Loss=0.00819]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:02<00:00,  2.50it/s, WMAPE=2.35, DICE=0.071, DPEAKS=48.8, Loss=0.00692]\n",
            "Validate Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:11<00:00,  6.66it/s, WMAPE=9.73, DICE=0.0808, DPEAKS=47.1, Loss=0.0659]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:04<00:00,  2.47it/s, WMAPE=2.28, DICE=0.0673, DPEAKS=44.8, Loss=0.00681]\n",
            "Validate Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.11it/s, WMAPE=485, DICE=0.0734, DPEAKS=47.6, Loss=2.45]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "early stopping: 3 epochs without improvement\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "train_loader, val_loader = create_train_valid_loaders(\n",
        "    f\"partition_{current_partition}_train.json\",\n",
        "    f\"partition_{current_partition}_val.json\",\n",
        "    \"partitions\",\n",
        "    batch_size=16,\n",
        ")\n",
        "best_model_path = os.path.join(\n",
        "    \"models\", f\"best_model_partition_{current_partition}.pth\"\n",
        ")\n",
        "last_checkpoint_path = os.path.join(\n",
        "    \"models\", f\"last_checkpoint_partition_{current_partition}.pth\"\n",
        ")\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    best_model_path,\n",
        "    last_checkpoint_path,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    num_epochs=50,\n",
        "    device=DEVICE,\n",
        "    early_stopping_patience=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validate Epoch test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1941/1941 [05:01<00:00,  6.43it/s, WMAPE=3.87, DICE=0.0736, DPEAKS=44.7, Loss=0.0118]\n",
            "Validate Epoch partition_3_train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.37it/s, WMAPE=3.33, DICE=0.0728, DPEAKS=45, Loss=0.0113]\n",
            "Validate Epoch partition_3_val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.34it/s, WMAPE=1.96, DICE=0.0714, DPEAKS=43.1, Loss=0.00634]\n"
          ]
        }
      ],
      "source": [
        "#Cargar mejor modelo de la particion actual\n",
        "model.load_state_dict(torch.load(best_model_path,weights_only=True))\n",
        "test_model(model,criterion,device=DEVICE,batch_size=16)\n",
        "create_next_partitions(current_partition,model,criterion,device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 1:   0%|          | 0/307 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:07<00:00,  2.41it/s, WMAPE=3.01, DICE=0.0907, DPEAKS=63.8, Loss=0.00851]\n",
            "Validate Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.20it/s, WMAPE=3.87, DICE=0.143, DPEAKS=100, Loss=0.0124]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model\n",
            "Epoch [2/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.37it/s, WMAPE=3.1, DICE=0.0946, DPEAKS=68.5, Loss=0.00892]\n",
            "Validate Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.17it/s, WMAPE=362, DICE=0.0801, DPEAKS=55, Loss=0.122]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.37it/s, WMAPE=2.68, DICE=0.0747, DPEAKS=53.3, Loss=0.00766]\n",
            "Validate Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.16it/s, WMAPE=2.21, DICE=0.0878, DPEAKS=48.6, Loss=0.0106]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model\n",
            "Epoch [4/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.36it/s, WMAPE=2.55, DICE=0.0734, DPEAKS=48.8, Loss=0.00727]\n",
            "Validate Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.14it/s, WMAPE=1.51e+4, DICE=0.0786, DPEAKS=52.1, Loss=32.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:09<00:00,  2.37it/s, WMAPE=2.54, DICE=0.0699, DPEAKS=48.3, Loss=0.00709]\n",
            "Validate Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.14it/s, WMAPE=3.39e+3, DICE=0.0779, DPEAKS=49.1, Loss=17.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:02<00:00,  2.51it/s, WMAPE=2.37, DICE=0.0672, DPEAKS=45.8, Loss=0.00672]\n",
            "Validate Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:11<00:00,  6.64it/s, WMAPE=260, DICE=0.068, DPEAKS=44.9, Loss=1.31]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "early stopping: 3 epochs without improvement\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validate Epoch test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1941/1941 [04:59<00:00,  6.49it/s, WMAPE=3.4, DICE=0.0858, DPEAKS=41.4, Loss=0.0158]\n",
            "Validate Epoch partition_4_train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:47<00:00,  6.41it/s, WMAPE=2.73, DICE=0.0857, DPEAKS=40.3, Loss=0.0178]\n",
            "Validate Epoch partition_4_val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:11<00:00,  6.43it/s, WMAPE=2.52, DICE=0.0857, DPEAKS=39.7, Loss=0.013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:02<00:00,  2.51it/s, WMAPE=2.71, DICE=0.0738, DPEAKS=53.7, Loss=0.00747]\n",
            "Validate Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.15it/s, WMAPE=5.84, DICE=0.0824, DPEAKS=54.9, Loss=0.0163]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model\n",
            "Epoch [2/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:05<00:00,  2.44it/s, WMAPE=2.61, DICE=0.0728, DPEAKS=51.6, Loss=0.00722]\n",
            "Validate Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.16it/s, WMAPE=935, DICE=0.0781, DPEAKS=54.6, Loss=1.09]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:06<00:00,  2.43it/s, WMAPE=2.5, DICE=0.067, DPEAKS=47.1, Loss=0.00683]\n",
            "Validate Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.25it/s, WMAPE=9.52e+3, DICE=0.0723, DPEAKS=57, Loss=0.268]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/50]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [02:05<00:00,  2.46it/s, WMAPE=2.46, DICE=0.0678, DPEAKS=46, Loss=0.00664]\n",
            "Validate Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.14it/s, WMAPE=1.34e+4, DICE=0.15, DPEAKS=92.6, Loss=76.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "early stopping: 3 epochs without improvement\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validate Epoch test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1941/1941 [05:12<00:00,  6.21it/s, WMAPE=5.65, DICE=0.0802, DPEAKS=41.3, Loss=0.016]\n",
            "Validate Epoch partition_5_train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:47<00:00,  6.51it/s, WMAPE=4.68, DICE=0.0801, DPEAKS=41.2, Loss=0.0135]\n",
            "Validate Epoch partition_5_val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:12<00:00,  6.15it/s, WMAPE=4.94, DICE=0.0805, DPEAKS=40.2, Loss=0.0139]\n"
          ]
        }
      ],
      "source": [
        "for current_partition in range(3,5):\n",
        "    best_model_path = os.path.join(\n",
        "        \"models\", f\"best_model_partition_{current_partition}.pth\"\n",
        "    )\n",
        "    last_checkpoint_path = os.path.join(\n",
        "        \"models\", f\"last_checkpoint_partition_{current_partition}.pth\"\n",
        "    )\n",
        "    train_loader, val_loader = create_train_valid_loaders(\n",
        "        f\"partition_{current_partition}_train.json\",\n",
        "        f\"partition_{current_partition}_val.json\",\n",
        "        \"partitions\",\n",
        "        batch_size=16,\n",
        "    )\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        best_model_path,\n",
        "        last_checkpoint_path,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        num_epochs=50,\n",
        "        device=DEVICE,\n",
        "        early_stopping_patience=3,\n",
        "    )\n",
        "    #Cargar mejor modelo de la particion actual\n",
        "    model.load_state_dict(torch.load(best_model_path,weights_only=True))\n",
        "    test_model(model,criterion,device=DEVICE,batch_size=16)\n",
        "    create_next_partitions(current_partition,model,criterion,device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
